\section{Speculative execution}\label{sec-haxl}

\Haxl~\citep{marlow2014haxl} is a framework for efficiently executing
code that fetches data from external sources, typically databases or
remote services. The \Haxl framework allows code written in a natural
style using \hs{Applicative} and \hs{Monad} combinators to run
efficiently, by automatically parallelising the data fetch operations
and batching together multiple fetches from the same data source.
\Haxl has been in use at Facebook, at scale, for several years now in
a system that proactively detects and remediates various forms of
abuse. \Haxl allows the engineers working on the anti-abuse code to
write clear and concise application logic, because the framework
abstracts away from the details of concurrency and efficient
data-fetching.

To illustrate the idea using a fragment of the example code from
\citep{marlow2014haxl}, suppose we are writing the code to render a blog into
HTML. The blog consists of a set of posts, where each post is
identified by a \hs{PostId}.  The data for the blog is stored in a
remote database.  The API for fetching the data from the database is
as follows:

\vspace{1mm}
\begin{minted}[xleftmargin=10pt]{haskell}
getPostIds     :: Haxl [PostId]
getPostContent :: PostId -> Haxl PostContent
\end{minted}
\vspace{1mm}

\noindent
We can fetch the set of all \hs{PostId}s using \hs{getPostIds}, and we
can fetch the content of one post using \hs{getPostContent}. To get
the content of all posts we could write:

\vspace{1mm}
\begin{minted}[xleftmargin=10pt]{haskell}
getAllPostsContent :: Haxl [PostContent]
getAllPostsContent = getPostIds >>= mapM getPostContent
\end{minted}
\vspace{1mm}

\noindent
Now, when we \hs{mapM}~\hs{getPostContent} we would really like the
database queries to happen in parallel, because there are no
dependencies between them. Furthermore, we might even be able to batch
up the queries into a single request to the remote database.

These optimisations are performed automatically by \Haxl, using a
special \hs{Applicative} instance that exploits the lack of
dependency between the two computations to explore the computations
and collect the data fetch operations that can be performed in
parallel or batched together. \Haxl also has a \hs{Monad} instance
that sequentialises operations as you would expect, but code written
to use \hs{Applicative} operations benefits from the automatic
concurrency. This optimisation is further exploited by using a
transformation on the monadic \hs{do} syntax to automatically use
\hs{Applicative} operators where possible \cite{marlow2016applicativedo}.

One of the key tools found to be useful in the kind of code written
using \Haxl at Facebook is the ``lazy'' conditional operators:

\vspace{1mm}
\begin{minted}[xleftmargin=10pt]{haskell}
(.||), (.&&) :: Haxl Bool -> Haxl Bool -> Haxl Bool
@\blk{x}@ .|| y = do b <- x; if b then return True else y
@\blk{x}@ .&& y = do b <- x; if b then y           else return False
\end{minted}
\vspace{1mm}

\noindent
These are typically used to improve performance by guarding slow
checks with faster checks.  For example, we might write:

\begin{minted}[xleftmargin=10pt]{haskell}
if simpleCondition .&& complexCondition then ... else ...
\end{minted}

\noindent
The idea is that \hs{simpleCondition} is quick to evaluate and
returns \hs{False} in a large proportion of cases, so that we can
often avoid needing to evaluate \hs{complexCondition}.

This does not require any additional extensions or special support in
\Haxl. But we also noticed that sometimes there is a pair of conditions
where neither is obviously faster than the other, yet we would still
like to benefit from bailing out early when the answer is known.
Therefore, \Haxl contains two more conditional operators \hs{pOr} and
\hs{pAnd} for ``parallel OR'' and ``parallel AND'':

\begin{minted}[xleftmargin=10pt]{haskell}
pOr, @\blu{pAnd}@ :: Haxl Bool -> Haxl Bool -> Haxl Bool
\end{minted}

\noindent
These have the behaviour that: (i)~both arguments are evaluated in parallel;
(ii)~the computation is aborted as soon as the answer is known, even if the
other argument is still being evaluated.

Data-fetches are not observable effects, so the parallelism is not
observable to the programmer (this is the property that \Haxl relies
on for the soundness of its parallel \hs{Applicative}
instance). However, \hs{pOr} and \hs{pAnd} are non-deterministic with
respect to exceptions: if an exception is thrown by either side, it
will be thrown by the computation as a whole immediately without
waiting for the other side to complete.  One could imagine an
alternative implementation which waits for the completion of the other
argument when an exception is raised; this would be deterministic, but
would be less efficient in the case of exceptions.

It should come as no surprise that \hs{pOr} and \hs{pAnd} can be
implemented using \hs{select}, indeed \hs{pOr}~\hs{=}~\hs{(<||>)} and
\hs{pAnd}~\hs{=}~\hs{(<&&>)} from Fig.~\ref{fig-library}. A \hs{Selective}
instance in terms of the \Haxl implementation from \citet{marlow2014haxl} is
given in Fig.~\ref{fig-haxl-select}.  For the purposes of the presentation
here we have renamed \hs{Fetch} to \hs{Haxl} and added support for
exceptions in the form of the \hs{Throw} cases.

% It would be nice to also show instances of Applicative and Monad.
\begin{figure}
\begin{minted}[fontsize=\small]{haskell}
instance Selective Haxl where
    select (Haxl x) (Haxl f) = Haxl $ do
        rx <- x
        case rx of
            Done (Right b) -> return (Done b)
            Done (Left  a) -> unHaxl (($a) <$> Haxl f)
            Throw e        -> return (Throw e)
            Blocked bx c   -> do
                rf <- f
                case rf of
                    Done f  -> unHaxl (either f id <$> c)
                    Throw e -> return (Blocked bx (select c (Haxl (return rf))))
                    Blocked bf d -> return (Blocked (bx <> bf) (select c d)
\end{minted}
\vspace{-2mm}
\caption{An implementation of \hs{Selective} instance for \Haxl.}
\label{fig-haxl-select}
\vspace{-4mm}
\end{figure}

There is one wrinkle with implementing \hs{pOr} and \hs{pAnd}
in terms of \hs{select}. Ideally, \hs{pOr} and \hs{pAnd} would be
symmetric: just as we can abort the right argument if the left
argument determines the answer, we should be able to abort the left
argument in the same way. Yet \hs{select} is inherently left-biased:
it requires that all the effects of the first argument are performed.
In~\S\ref{sec-alt-symmetric} we consider an alternative combinator
related to \hs{select} that allows this kind of symmetry to be expressed.

We have prototyped an implementation of \Haxl with the \hs{Selective}~\hs{Haxl}
instance, which allowed us to reuse generic selective combinators
\hs{<||>}, \hs{<&&>}, \hs{anyS} and \hs{allS} instead of providing custom
implementations for conditional operators operators \hs{pOr} and \hs{pAnd} and
their generalisations on lists. This case study highlights the fact that
selective functors are useful not only in the static context, but in the dynamic
context too, by allowing us to benefit from speculative execution.

\subsection{Results}

We mentioned above that \hs{pOr} and \hs{pAnd} are effective when the
relative size of the conditional computations is unknown, so
evaluating them in parallel with early exit is an effective
alternative to either sequencing them manually (with \hs{Monad}) or
evaluating them in parallel to completion (with
\hs{Applicative}). This argument becomes even more compelling as the
set of conditions to evaluate grows: imagine trying to efficiently
sequence a set of ten or more conditions, and then repeating the
exercise every time the set changes.

For this reason, in \Haxl we found that list operations built on top
of \hs{pOr} and \hs{pAnd}, which in this paper we call \hs{andS} and
\hs{allS} (see Fig.~\ref{fig-library}), offer an important balance
between performance and maintainability that is not provided by the
\hs{Applicative} or \hs{Monad}-based combinators.

One could construct examples to demonstrate arbitrarily large
performance gains from using \hs{pOr} and \hs{pAnd}, however that
would not be particularly useful. Perhaps more useful would be a
real-world measurement showing how much performance was improved in an
actual application --- but again, the value of that would depend to a
large extent on how the application uses \hs{pOr} and \hs{pAnd}, and
unfortunately the application code in our case is
proprietary. Therefore instead we offer this anecdote: we first
introduced a use of \hs{pOr} to solve some performance issues in a
complex production workload where we had long chains of conditionals
that were difficult to optimise by hand, and \hs{pOr} resulted in
significant performance improvements.
